---
layout: post
title: Understanding the Impossible 
---

2019 started with a bang at work. My school organized a research week with a series of distinguished research
talks delivered by world-class researchers. I hardly came away from any talk without learning anything new or
interesting. The first day covered distributed systems, in glorious theoretical depth and practical
complexity. Having only recently gotten less scared of this field, I left Valerie King's presentation with
wholly new understanding of the FLP theorem. Which is to say that my previous, though unstated, view of the
theorem was wrong. Well, not as much wrong as incomplete. And that prompted me re-reading some papers, then
spent several days wrestling with the original FLP paper. This blog is intended to capture what I understood
**so far**. Knowing how subtle distributed systems are, it will not be the last on FLP and consensus protocols. 

## FLP and its implication
At a high level, FLP theorem says that consensus in an asynchronous network is impossible in the presence of
node failure. This holds even for a **single crash failure**. We are reminded that crash failure is the most
simple form of failure in distributed systems. The FLP result, as my initial understanding goes, has this
implication: 

>In practical terms, we cannot simultaneously have safety (all non-faulty nodes agrees/decides on the same
>value) and liveness (the protocol terminates with a decision). Therefore, every consensus protocol must either
>sacrifice one property, or relax assumption about network asynchrony. 

Most state-of-the-art consensus protocols follow the second strategy. PBFT, for example, assumes *partially
synchronous* networks in which there exists an unknown bound on message delivery. PBFT achieves safety even in
fully asynchronous network condition, and liveness under period of synchrony, as permitted by FLP. 

So far, this view of FLP is not wrong. But it is not the whole truth. What I've been missing is that 

> FLP concerns only **deterministic** consensus protocols.

(This fact is stated so clearly in the original paper that I have only myself to blame for my initial, shallow
understanding of one of the most important results in computer science.) 

Now before I go and repent for this gross oversight, it is worth mentioning that I did sense something amiss
when reading Andrew Miller's HoneyBadger protocol (HBFT) which claims to work in an asynchronous systems. Having
failed to understand all its details, I drew a conclusion that HBFT circumvented FLP by sacrificing safety: it
achieves agreement only probabilistically. Well, now that was wrong!

A deterministic consensus protocol, as properly defined in FLP, means that nodes in the protocol  behave
deterministically. Determinism is a property of of node behavior, not of the final outcome. More specially, a
node output (its decision) is entirely determined by the sequence of input events the node receives. All
protocols that I had so far known of are deterministic. PBFT, Raft and Paxos, for examples, contains no
randomness. When they receives a message, there path of execution is completely deterministic. Of course,
different orders of inputs lead to different paths; but again that mapping of sequence of input to execution
(and subsequently to output) is completely deterministic. 

### Non-deterministic protocol
So what does a non-deterministic consensus protocol looks like. Well, opposite of a deterministic thing is a
*randomized* (or probabilistic) thing. A randomized algorithm's output depends not only on its external input,
but also on a random coin which is flipped during its execution. So, given two exactly the same inputs, the
algorithm may output two different values.  

FLP theorem does not apply to randomized protocol. Viewed this way, it is a positive result, giving
researchers another way to engineer practical consensus protocols. There are only a small numbers of such
algorithms, the most famous one is by Ben-Or in 1983, and they all share the same structure as follows:

* Each node has a source of randomness. Some algorithms allow the node to have its private, local coin; other
optimize for cases when there exists a global coin. All algorithms proceed in many rounds of voting.  

* Each node broadcasts its value to the network, and waits for $$n-f$$ messages to come back. This constitutes
a round of execution, after which the node moves on to another round. Note that it cannot wait for more than
$$n-f$$ messages, due to network asynchrony. 

* There are some thresholds $$t$$ defined in $$n$$ and $$f$$. If the node receives more than $$t$$ messages
with the same value, it can decide immediately. 

* There are some other threshold $$t'$$, below which the node tosses the coin and vote for that random value
in the next round.  

The probability that a node will decide at round $$r$$ is of this form 

$$P = 1 - (1-x)^r$$

which means $$P=1$$ as $$r \to \infty$$. State-of-the arts randomized protocols compete with each other by
lowering the **expected** number of rounds by which the nodes decide. Many trade fault tolerance (the ratio of
$$t$$ against $$n$$) for constant and low expected number of round. 

**Why would it work?** One direct question to ask is why, intuitively, non-determinism helps circumventing
FLP. The core idea here is that in asynchronous networks, we assume the worst, that is the adversary has
total control over message delivery with the only constraint that it will eventually deliver messages. In
deterministic settings, sequence of messages is extremely important. In fact, the adversary in FLP craftily
delay messages so that the nodes can never decide. In contrasts, the effect of message delivery (and order) is
less important in non-deterministic setting, because the node behavior is also dependent on some randomness
that the adversary cannot control. In other words, the adversary exerts less power in this setting, and
therefore cannot always prevent the algorithm from reaching agreement. 

---

## FLP model
Now let us formally describe FLP. As many classic distributed systems papers, FLP is deceptively short and
sweet. But only by re-reading it so many times, together with working backwards, that I am able to feel how
the proof works. 

### The node
Each node is fully specified by its **state** and a **transition** function. The state consists of the propose
value, the final output, and any temporary variables during execution. The transition function is the meat. It
is invoked by a receiving a message from the network: 

* Given the message and current state as input, it deterministically moves the node to a new state. 
* It may send many messages to the network during execution of the transition function. 

### The network
Nodes communicate through the network which is modeled as a centralized buffer. Controlled by the
adversary, it consists of messages exchanged between node. A message is of the form $$(p,m)$$ where $$p$$ is
the message destination, $$m$$ is the content. 

An **asynchronous network** is characterized by the fact that the adversary controls *when* a message is
delivered. In particular, the adversary can reorder messages in the buffer and insert arbitrary number of
empty (or null) messages to the buffer. For instance, instead of delivering a message $$(p,m)$$ to $$p$$, the
adversary can simply deliver $$(p,\emptyset)$$. In either case, $$p$$'s transition function is invoked. The
real message $$m$$ can be delayed for arbitrarily long. 

However, delayed messages are not the same as lost.  The asynchronous network does provide a form of
reliability: over infinite period of time, $$(p,m)$$ will eventually be delivered. More specifically, if $$p$$
infinitely asks the buffer for a message, and there exists a $$(p,m)$$ in the buffer, $$p$$ will receive it.
The problem here is the word *infinity*, which in this case means there is no bound on the delay. In contrast,
in partially synchronous settings, there is an unknown, yet finite period after which $$(p,m)$$ is delivered. `    


### Terminologies

---

## FLP proof


