---
layout: post
title: History of the Impossibles - CAP and FLP 
---

My undergraduate course in distributed systems was a disappointment. I started the module (in winter 2006)
anticipating highly stimulating sessions, but was quickly let down by the materials. The textbooks were not
the best, and the lecturers were clearly not passionate about the field. Times and times again over the next
ten years I found blind spots after blind spots in my understanding. It was frustrating to encounter a
(design) problem and have to work my way backwards trying to reduce it to known principles and guidelines. As
much as this problem-solving process helps remembering the principles better, I really wish these fundamentals
were (better) taught in University.

This post will be about two principles in distributed systems underlying most of modern day designs. One is
old but so well formalized that it becomes a theorem. Another, more recent and rather informal, crystallises
wisdoms in designing distributed systems, from which followed many debates and extensions.  

# Distributed System Models
Distributed systems are built over a network of nodes, so it is important to understand the network model,
i.e. the assumptions of the network links. 

* **Asynchronous model:** the delay when sending a message from one node to another is finite but can be
unbounded.  As a consequence, a node cannot tell if the message is lost or simply being delayed. In other
words, a node cannot use **time-out/ping** as a failure detector. This model reflects worst-case
communication, but it is not too unrealistic to be useful. 

* **Synchronous model:** delay in sending a message is bounded, and finite by extension. This reflects more
accurately what we have today: the delay bound can be accurately _approximated_ by sampling. By using this
bound, one can tell with high confidence whether the message is lost. Note that in theory it is still not
possible to have an absolute bound, in practice a sufficiently long delay can be safely treated as message
loss.   

<div class="highlight" style="font-size:18px"> 
It is easy to see that asynchronous models subsume synchronous ones. Thus, anything that <b>works</b> under the former
will do so under latter. Likewise, anything that <b>does not work</b> in the latter will also fail to work in the
former.  
</div>

### Network Partition - How the Network Fail
The difference between asynchronous and synchronous model can be made clearer under network failure. When a
link fails, messages sent on that link is lost forever. For simplicity, we can assume further that the link failure
leads to two isolated sub-networks (or **partitions**) that cannot communicate with each other, i.e. messages between
two nodes in two partitions are never delivered. 

Let us image that a node in the system knows that there **may be** partition, and its tasks is to find out
if it really exists (and if yes, which of its neighbors are in the same partition with itself). The task, in
  other words, is to build a failure detector.  ![Failure Detector](../images/failuredetector.jpg)

* In an asynchronous network, since delay can be unbounded it is not possible to tell from a long delay whether the
delay is due to failure or due to the network being slow. In the figure above, the yellow node cannot
distinguish between (a) and (b) in finite time. It can wait for very long time $$\delta$$ for the response and
conclude that the blue node is in a different partition, but it may just happen that the message is delayed
for $$\delta' > \delta$$. _Note subtly that even if the yellow node **knows** a priori that parition exists,
  i.e. it knows it's in (a), it still cannot tell which nodes are in the same paritition._

* In a synchronous network, the node can simply wait for $$\delta$$ time units where $$\delta$$ is the bounded round
trip time. When an acknowledgement is delivered within $$\delta$$, the other node is indeed reachable and
hence in the same partition. However, when this time passes and no response is received, it can be safely
assumed that the other node is in another partition. In the figure above, (a) and (b) are distinct.  

Thus, detecting network partition in asynchronous network is straightforward in a synchronous network, but
impossible in asynchronous network. 

### How Nodes Fail
Having understood how the network behaves, it is equally important to be explicit about how the nodes fail. Things fail
all the time, but not all failures are born equal. There are roughly two classes of failure:

* **Crash failure:** a node simply stops working. It may or may not come back on. 
* **Byzantine failure:** a node does not stop working, but it behaves arbitrarily. This model encompasses a large
classes of misbehavior, either due to unintended bugs or deliberate malice. 

To a communicating node, crash failure is much easier to deal with since we the failed node can be in only one state.
However, Byzantine failure is notoriously difficult, because the failed node can be in any arbitrary state. In some
cases, such behavior adds noises to the overall execution. But in others it can lead to harmful states that
compromises system invariants. 

We are now ready to face the impossibles. 

---

# FLP - Agree to Disagree
In 1985, Michael J. **F**isher, Nany A. **L**ynch and Michael S. **P**aterson published a seminal paper that
is later referred to as **FLP**. Their paper concerned **_distributed consensus_** problem: a number of
autonomous nodes agree on a proposed value. They considered 3 properties of any solution to this problem. 

* **Safety:** the nodes agree on a valid value (proposed by one of the node).
* **Liveness:** the nodes _eventually_ reach agreement, i.e. the system must make progress. 
* **Fault-tolerance:** the solution must work when there _may_ be failure in the network. 

The FLP result can be roughly stated as: one can have _at most 2 out of 3_ properties above. In other words,
no algorithms can be safe, live and fault-tolerant at the same time. 

<div class="highlight" style="font-size:18px"> 
<b>FLP Theorem:</b> In an asynchronous network, it is not possible to achieve safety and liveness when there may be
one crash failure. 
</div>

The suprising result here is that only one potential crash failure is needed to the impossibility to take
effect. Note that _potential failure_ is enough, meaning that even when there is no failure, as in Figure
1(b), the algorithm still cannot be safe and live at the same time. Now this may sound counter-intuitive: if
no failure is present, then the algorithm does not have to be fault tolerance, hence the other two properties
could be achieved, couldn't it? 

<div class="highlight" style="font-size: 20px">
The key idea here is that fault-tolerance is a property of the algorithm, whereas failure is the property of
the network. A fault-tolerant algorithm A does not know if there is failure, only that it must be designed to
handle failure. Particularly, A is designed to work in both Figure 1(a) and 1(b).  In contrast, a
fault-intolerant algorithm B only works in Figure 1(b).  
</div>

As a consequence, even when there is no failure, $$\cal A$$ still takes up the fault-tolerance property. If
$$\cal A$$ can detect when failure is not presence, it can switch to another version that achieve both safety
and liveness. But as we already see in Figure 1, it's not possible to detect failure in an asynchronous
network, hence $$\cal A$$ mus always be fault-tolerant. 

# CAP - Separation is Considered Harmful 
In 2000, Eric Brewer in an invited talk at PODC, proposed a principle which is later referred to as the CAP theorem.
Now, Brewer never called it a _theorem_, and the unfortunate wording subsequently fueled confusions surrounding CAP.
Initially a _conjecture_, CAP concerns **_distributed, replicated storage_** systems whose role is to present a
single-copy illusion to end users. I will defer the discussion of how the problem scope is not same as distributed
consensus to later. CAP considers (well, it comes from the initials of) 3 properties of a distributed, replicated
storage system. 

* **Consistency:** consistency is the guarantees of how read/write behave in a distributed shared data systems. This is
a concern when there are multiple, concurrent writers and nodes may cache data. There are multiple level of consistency,
but in CAP a strongest one --- linearizability --- is assumed: read must see the latest value of write. 
* **Availability:** intuitively it means the system should be able to serve requests. But Brewer was not very explicit
on his definition of availability, and it became root of later confusion. For now, I stick with the prevailing
intuition.   
* **Partition-tolerance:** similar to fault-tolerance in FLP, this means the system must handle network partition
(regardless of whether there actually is a partition or not).  

The CAP conjecture is roughly stated as follows: a distributed replicated storage system can have at most 2 out of 3
properties above. In other words, one cannot design a system which is consistent, available while being fault tolerant
at the same time. This conjecture at that time was not ground breaking, as it basically distills what system designers
had known and coped with. And it was quite intuitive: when there is a partition, the system must either refuse requests
(non-available) or return stale (non-consistent) data. When there is no partition, replicas can communicate and there
exists a design returning consistent data in finite time (image a primary-backup design). Careful readers may notice
a discrepancy of this intuition to the original conjecture: "there is a partition" is not the same as "partition
tolerance". The former is a property of the network, latter of the system itself.     

---

# FLP vs CAP - Chalk and Cheese
![Trade-offs](../images/flpcap.jpg)
Figure 2 summarizes the take-aways from FLP and CAP. Both are impossibility results, both suggest that only 2 out of
some 3 properties are possible. Both are useful in helping designers filter out impossible designs at early stage.
Their relation (or equivalence) elicits lengthy discussion [1]. Here I would like to discuss few notable differences. 

| Properties vs. Results | FLP | CAP |
|-----|-----|-----|
|Problem scope | distributed consensus | replicated storage |
|Failure | node crash fails | network fails | 
|Formalization | FLP | Gilbert \& Lynch | 
|Solution| (partially) synchronous | (partially) synchronous | 

### Problem scopes
**Consensus vs. Replicated Storage.**

**Node failure vs. Network Partition.**
Replicas vs independent parties
Partition vs failure 

### Formalization
Proof. 

### Impact and Implication:
FLP: SL, SF, LF examples
CAP: CA, AP, CP examples

---

## FLP Defied
Defying FLP: probabilistic failure dectector, partially synchronous network. 

## CAP Evolved
CAP falling out of favor: 
+ Problem with the formalization
+ Not binary decision.
+ Latency to model avaialability: PACELC

## Concluding Notes
Conclusion: FLP lives, CAP can be safely retired. 
