---
layout: post
title: History of the Impossibles - CAP and FLP 
---

My undergraduate course in distributed systems was a disappointment. I started the module (in winter 2006)
anticipating highly stimulating sessions, but was quickly let down by the materials. The textbooks were not
the best, and the lecturers were clearly not passionate about the field. Times and times again over the next
ten years I found blind spots after blind spots in my understanding. It was frustrating to encounter a
(design) problem and have to work my way backwards trying to reduce it to known principles and guidelines. As
much as this problem-solving process helps remembering the principles better, I really wish these fundamentals
were (better) taught in University.

This post will be about two principles in distributed systems underlying most of modern day designs. One is
old but so well formalized that it becomes a theorem. Another, more recent and rather informal, crystallises
wisdoms in designing distributed systems, from which followed many debates and extensions.  

## Distributed System Models
Distributed systems are built over a network of nodes, so it is important to understand the network model,
i.e. the assumptions of the network links. 

* **Asynchronous model:** the delay when sending a message from one node to another is finite but can be
unbounded.  As a consequence, a node cannot tell if the message is lost or simply being delayed. In other
words, a node cannot use **time-out/ping** as a failure detector. This model reflects worst-case
communication, but it is not too unrealistic to be useful. 

* **Synchronous model:** delay in sending a message is bounded, and finite by extension. This reflects more
accurately what we have today: the delay bound can be accurately _approximated_ by sampling. By using this
bound, one can tell with high confidence whether the message is lost. Note that in theory it is still not
possible to have an absolute bound, in practice a sufficiently long delay can be safely treated as message
loss.   

<div class="highlight" style="font-size:20px"> 
It is easy to see that asynchronous models subsume synchronous ones. Thus, anything that <b>works</b> under the former
will do so under latter. Likewise, anything that <b>does not work</b> in the latter will also fail to work in the
former.  
</div>

### Network Partition - How the Network Fail
The difference between asynchronous and synchronous model can be made clearer under network failure. When a
link fails, messages sent on that link is lost forever. For simplicity, we can assume further that the link failure
leads to two isolated sub-networks (or **partitions**) that cannot communicate with each other, i.e. messages between
two nodes in two partitions are never delivered. 

Let us image that a node in the system knows that there exists partition, and its tasks is to find out which of its
neighbors are in the same partition with itself. The task, in other words, is to build a failure detector. 

* In an asynchronous network, since delay can be unbounded it is not possible to tell from a long delay whether the
delay is due to failure or due to the network being slow. Thus, the node cannot reliably decide (in finite time) which
nodes are in its partition. Worse still, the node cannot even tell if there is partition. 

* In a synchronous network, the node can simply waits for $$2\delta$$ time units. When an acknowledgement is delivered
within $$2\delta$$, the other node is indeed reachable and hence in the same partition. However, when no response is
received within $$2\delta$$, it can be safely assumed that the other node is in another partition. 

Thus, detecting network partition in asynchronous network is straightforward in a synchronous network, but impossible in
asynchronous network. 

### How Nodes Fail
Having understood how the network behaves, it is equally important to be explicit about how the nodes fail. Things fail
all the time, but not all failures are born equal. There are roughly two classes of failure:

* **Crash failure:** a node simply stops working. It may or may not come back on. 
* **Byzantine failure:** a node does not stop working, but it behaves arbitrarily. This model encompasses a large
classes of misbehavior, either due to unintended bugs or deliberate malice. 

To a communicating node, crash failure is much easier to deal with since we the failed node can be in only one state.
However, Byzantine failure is notoriously difficult, because the failed node can be in any arbitrary state. In some
cases, such behavior adds noises to the overall execution. But it can also lead to harmful states that compromises
system invariants.   


## FLP - Agree to Disagree
FLP: consensus: safety, liveness, fault-tolerance is impossible. 

fault-tolerance is NOT the same as actually having failure

## CAP - Separation is Bad
CAP: consistency, availability, partition-tolerance is impossible. 

Replicas vs. independent parties
parition-tolerance is NOT the same as the network has partition or not. 

## FLP vs CAP - Chalk and Cheese
Chalk and Cheese:

+ Formalization: execution traces, liveness = availability. State theorems 
+ Proof. 

+ Impact/Implication:
FLP: SL, SF, LF examples
CAP: CA, AP, CP examples

## FLP Defied
Defying FLP: probabilistic failure dectector, partially synchronous network. 

## CAP Evolved
CAP falling out of favor: 
+ Problem with the formalization
+ Not binary decision.
+ Latency to model avaialability: PACELC

## Concluding Notes
Conclusion: FLP lives, CAP can be safely retired. 
