---
layout: post
title: Subtle Details in PBFT
---
To be updated ...

<!---
Almost a year has gone by since I sat down and decided to take on Byzantine fault tolerance protocols,
starting with its poster child PBFT. Despite countless reprintings and re-readings the OSDI'99 version of the
paper, I never stop learning new things at every reread. This is party exacerbated by the fact that
Hyperledger Fabric, an open source permissioned blockchain system, contains a Go implementation of PBFT which
serves as the basis for truly understanding the protocol. Discrepancies between the implementation and the
paper bring the protocol's internal intricacies out to the surface. The following summarizes few subtleties I
discovered. 

## Network failure model
Byzantine fault tolerant protocols tolerate at most $$f$$ Byzantine failures. In a distributed system,
however, there are two types of failures: node and network. It is important to distinguish them, especially
since they determine guarantees about the protocol's safety and liveness. 

Node failure means a node (or server, peer, entity, etc.) behaves arbitrarily. This captures the strongest
adversary model. A network failure refers to network partitions which can last for an unbounded amount of
time. It can be quantified as the number of nodes being isolated from the rest of the network, although they
can still communicate with each other. 

Given this distinction, what are being counted toward $$f$$? In PBFT
*  $$f$$ refers to node failures. The protocol **guarantees safety upto $$f$$ node failures**. Safety
is independent of network failure. That is, even if the network is severely partition, namely more than $$f$$
nodes are isolated (*but across all partitions there are still fewer than $$f$$ node failures*), the protocol
is still safe. 

* However, **liveness** is only guaranteed with fewer than $$f$$ total failures, i.e. counting both node and
network failures. This means at least $$2f+1$$ nodes must be reachable. 

Note that safety being independent of network failures is a strong guarantee, and it is common in most variants of
PBFT. Not until  Liu et al. recent work, namely XFT, is it relaxed in a way that separates out node and
network failures. 

## Quorum and weak certificates

### A little digression on quorum systems
**What is a quorum?**

Given a set of node $$U$$, a quorum system is a set of subsets of $$U$$ (or quorums) in which **any two quorums
intersect**. More formally, $$Q$$ is a quorum system defined as $$Q \subseteq \mathbb{P}(U) \ \wedge \ \forall q_1, q_2
\in Q [ q_1 \ \cap \ q2 \neq \emptyset ]$$ 


**Examples**

A set of majority subsets in $$U$$ is a quorum system. In fact, it is the most common instance of quorum systems,
found most often in fault-tolerant protocols. Paxos, in particular, relies on quorums of size $$f+1$$ in network of
$$2f+1$$ nodes. However, there are other examples whose quorums are not majority set. 

![quorums](../images/quorums.jpg)

The figure above illustrates two quorum systems derived from $$U$$ of 16 nodes. The first system, called majority
quorum, is made up of majority-set quorums wherein each quorum is of size 9. The second, called grid quorum, consists of
quorums of size 7, but each pair always intersects at least 2 nodes. There are $$|U|$$ quorums in this example, where
$$q_{ij}$$ comprises node in row $$i$$ and column $$j$$. 

masking
dissemination quorum
grid vs. majority quorums

**Quorum vs. Paxos**

### Quorum vs. weak certificate

Latest value vs. correct value

Votes vs states

3f+ 1 is optimal
- Cannot be fewer. Relaxing it means relaxing the adversary model. 
- More than that means bigger quorum size, which incurs more network overhead. 

## Stable checkpoints
-->
