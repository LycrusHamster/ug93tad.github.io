---
layout: post
title: Another post about differential privacy 
---

In the wake of [Apple's
announcement](https://www.wired.com/2016/06/apples-differential-privacy-collecting-data/) that they
incorporating differential privacy in many of the Apple products, I think it's high time to continue my quest
to have a firm grasp on differential privacy. I wrote an introductory note on differential privacy in my old
blog [1], but it was over 3 years ago. One could be forgiven in thinking that I should have become an expert
by now. Yeah, no. 

It was typical Apple not disclosing any information that would have justified the move as visionary, and that would
also have satisfied many security researchers. There are two ways to see it, and they are not mutually
exclusive. First, it's testament to how the technology has matured that it would soon become the norm. Second,
there are tremendous risks in adopting this technology without transparency, as it is teemed with
non-technological trade-offs. The most important consideration is how to trade privacy for utility. Of course
the fact that Apple even bring this consideration into a table is hugely encouraging, but this effort would be
but all for nothing if they did not make their decisions public. It is essential for the users to understand
the risks before sharing, and concealing them would completely offset the benefit of differential privacy. 

Though papers on differentially private algorithms and mechanisms still fill me with dreads, I have started to
comprehend and appreciate some of the very cool properties of differential privacy, thanks to two awesome
lecture notes I found online [2, 3]. The rest of this article I will try to articulate these properties as
much as I understand them. 

## Definition
Differential privacy is only applicable to *randomized algorithm* $$\cal{A}(\cal{D}) = \cal{Y}$$ ($$\cal{Y}$$ can
be a probability space, hence the correct formula is $$\cal{A}(\cal{D}) \in \cal{Y}$$). It means applying $$\cal{A}$$
on the same input will give different results, based on the whatever randomness embedded in $$\cal{A}$$ (or its coin
toss).  It's easy to image that $$\cal{D}$$ is the database of $$n$$ rows, and $$\cal{Y}$$ is the space of
vectorized results $$\langle y_1, y_2, .., y_m \rangle$$.  

**Ideal privacy.** We want $$\cal{A}(.)$$ to not reveal information about a given individual $$x$$, given
arbitrary auxiliary information. This means _publishing_ the result of $$\cal{A}$$ does not violate any
privacy. This, however, is impossible to achieve. A counter example is that $$x$$ is an associate professor,
and $$\cal{A}(.)$$ on the database of university staff reveals that an average salary of an associate
professor is (approximately) $$100K$$. Thus, we know that $$x$$ salary is $$100K$$ in average --- new
information being revealed as the result is published.  

**Relaxed (differential) privacy.** The ideal privacy is impossible, meaning that performing an analysis task
on the data inevitably leads to leakage. Differential privacy attempts to relax the requirement in order to
beat the impossibility result. It requires analysis result to be almost the same whether $$x$$ is in
$$\cal{D}$$ or not. It means that $$x$$ can opt out of the analysis without affecting the result. It also
means that $$x$$'s privacy is not decreased if she is in $$\cal{D}$$. Hence the name _differential_. 

**Formal definition.** _Algorithm $$\cal{A}: \mathbb{D} \to \mathbb{Y}$$ is $$\epsilon$$-differentially
private if for all $$\cal{Y} \in \mathbb{Y}$$ and $$\cal{D}_1, \cal{D}_2 \in \mathbb{D}$$ where
$$\cal{D}_1$$ and $$\cal{D}_2$$ differs by at most 1 entry, then:_

$$
\frac{Pr[{\cal A}({\cal D}_1) = {\cal Y}]}{Pr[{\cal A}({\cal D}_2) = {\cal Y}]} \leq e^\epsilon \qquad \quad (1)
$$

**Different notations.** Equation $$(1)$$ can be expanded or re-written as in the following ways:

+ When $$\epsilon$$ is small, $$e^\epsilon \approx 1+x$$. Thus:

$$
1 - \epsilon \leq \frac{Pr[{\cal A}({\cal D}_1) = {\cal Y}]}{Pr[{\cal A}({\cal D}_2) = {\cal Y}]} \leq 1 +
\epsilon \qquad \quad (2)
$$

+ Because $$Pr[f(.) = y \;\vert\; X=x ] = Pr[f(x) = y]$$, we have:

$$
\frac{Pr[{\cal A}(.) = {\cal Y} \;\vert\; {\cal D} = \cal{D}_1]}{Pr[\cal{A}(.) = \cal{Y} \;\vert\; 
\cal{D} = \cal{D}_2]} \leq e^\epsilon \qquad \quad (3)
$$

Differential privacy written in Equation (3) be stated as: the probably of $$\cal{A}(.)$$ outputting the same value
_given_ the input $$\cal{D}_1$$ is _similar_ to that given the input $$\cal{D}_2$$.


## Security equivalence
The formal definition (Eq. 2) expresses the bound of leakage in information theoretic sense, which is useful in most
statistical analysis. Most mathematicians or statisticians can work comfortably with this definition. For people coming
from a security background like me, a definition based on a security game would be easier to grasp. And I was glad to
finally find such a definition in [2]. It was truly refreshing seeing the same problem in a new light. 

The game, as usual, involves a challenger (Alice) and an adversary Bob. It proceeds as follows:

1. Alice generates $${\cal D} = \langle x_1,x_2,..,x_n \rangle$$.  

2. Alice sends $$\cal{A}$$, $${\cal D}' = \langle x_1,x_2,..,x_{n-1} \rangle$$ and $$\cal{Y} = {\cal A}({\cal D})$$ to Bob.

3. Bob guesses $$x_n$$. 

4. Bob wins if it can guess $$x_n$$ correctly with a probability __greater__ than a random guess. 

The winning probability of Bob is the level of privacy decrease that $$x_n$$ suffers, i.e. the advantage the adversary
would have had in identifying $$x_n$$ in the dataset. We can prove that if $$\cal{A}(.)$$ is $$\epsilon$$-differentially
private, the probability of Bob winning the above game is bound by $$\epsilon$$. 

For simplicity, let $$n=k=1$$ and $$y = \cal{A}(x) \in \{0,1\}$$ for $$x \in \{0,1\}$$. Furthermore: 

1. (**A1**) Alice chooses $$x$$ by flipping an unbiased coin, i.e. $$Pr[x=0] = Pr[x=1] = \frac{1}{2}$$. 

2. (**A2**) $$Pr[{\cal A}({0}) = 0] > Pr[{\cal A}(1) = 0]$$. 

The best guessing strategy by Bob is maximum likelihood estimation, i.e.

$$
guess \leftarrow argmax_j Pr[{\cal A}(j) = y] \qquad (4)
$$

Combining Eq.4 with assumption A2 gives us $$guess = y$$. 

Thus:

$$
\begin{align*}
Pr[\text{Bob wins}] & =  Pr[\text{guess}\ 0, x_n=0] + Pr[\text{guess}\ 1, x_n=1] \\
& = \frac{1}{2} Pr[\text{guess}\ 0 \,\vert\, x_n=0] + \frac{1}{2} Pr[\text{guess}\ 1 \,\vert\, x_n=1] \\
& = \frac{1}{2} Pr[y = 0 \,\vert\, x_n=0] + \frac{1}{2} Pr[y = 1 \,\vert\, x_n=1] \qquad \\
& = \frac{1}{2} Pr[{\cal A}(0) = 0] + \frac{1}{2} Pr [{\cal A}(1) = 1] \\ 
& = \frac{1}{2} + \frac{1}{2} (Pr[{\cal A}(0) = 0] - Pr [{\cal A}(1) = 0]) \\ 
& \leq \frac{1}{2} + \frac{\epsilon}{2} \qquad (5)
\end{align*}
$$

Eq.5 suggests that if $$\epsilon$$ is small, so is the leakage. In particular, if $$\epsilon = neg(.)$$ for a negligible
function $$neg(.)$$, the definition reduces to the _indistinguishibility_ notion found in traditional cryptography.
In practice, however, $$\epsilon$$ is quite substantial (in the range of $$(0.1, 10]$$) for the analysis to be useful.
Thus, there is a real risk of decrease in privacy when an individual is included in the data.  


## Properties
The definition of differential privacy implies three key properties that are instrumental in designing
differential private systems.

### Sequential composition
When a differentially private algorithm (analysis) is run $$k$$ times over the same dataset, the privacy
deteriorates. The consequence is that if the analyst doesn't want the privacy to get worse (and eventually
lost), he should bound the number of runs. _Publishing_ the result is a special case when the analysis is done
once then the data is discarded. 

Formally, _let $${\cal A}_i$$ for $$ 1 \leq i \leq k$$ be independent $$\epsilon_i$$-differentially private
algorithms. Then the algorithm $${\cal B}$$ made up from a sequence of $${\cal A}_i$$, i.e. $${\cal B} = ({\cal
A}_1, .. {\cal A}_k)$$ is $$\sum_i \epsilon_i$$-differentially private._

**Proof.** For any sequence of $$\langle {\cal Y}_1, {\cal Y}_2, .., {\cal Y}_k \rangle$$, and $${\cal D}_1$$
and $${\cal D}_2$$ different by at most 1 entry, we have:

$$
\begin{align*}
Pr[{\cal B}({\cal D}_1) = {\cal Y}_k] & = Pr[{\cal A}_1({\cal D}_1) = {\cal Y}_1, .., {\cal A}_k({\cal D}_1)=
{\cal Y}_k] \\
& = \prod_i Pr[{\cal A}_1({\cal D}_1) = {\cal Y}_1] \\
& = \prod_i e^{\epsilon_i}Pr[{\cal A}_i({\cal D}_2) = {\cal Y}_i] \\
& = e^{\sum_i \epsilon_i} \prod_i Pr[{\cal A}_i({\cal D}_2) = {\cal Y}_i] \\
& = e^{\sum_i \epsilon_i} Pr[{\cal B}({\cal D}_2) = {\cal Y}_k] \\
\end{align*}
$$

In practice, $$\epsilon$$ is called _privacy budget_, and the lower it is the better for privacy. An
implication of composition is that _every time_ you make a pass through the data with a
$$\epsilon$$-differentially private algorithm, the overall privacy budget is increased by $$\epsilon$$. A
common practice is to fix the budget $$\epsilon$$ at the beginning and ration it out to sub-analyst tasks. Once the
budget is exhausted, discard the data. 

### Partitioning
Note that sequential composition property considers multiple algorithms computing over the entire dataset.
When different algorithms touch different part of the data, then we have better bounds in terms of privacy
budget.  

### Post processing

## Mechanisms

### No curator - randomized response

### Trusted curator 

+ Interactive vs. non-interactive
+ Laplace
+ Histogram, counts
+ Linear regression


[1] https://anhdinhdotme.wordpress.com/2013/03/26/on-differential-privacy/

[2] https://www.acsu.buffalo.edu/~gaboardi/teaching/cse711Spring2016/Lecture1.pdf

[3] http://people.eecs.berkeley.edu/~sltu/writeups/6885-lec20-b.pdf
