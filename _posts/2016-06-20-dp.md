---
layout: post
title: Another post about differential privacy 
---

In the wake of [Apple's
announcement](https://www.wired.com/2016/06/apples-differential-privacy-collecting-data/) that they
incorporating differential privacy in many of the Apple products, I think it's high time to continue my quest
to have a firm grasp on differential privacy. I wrote an introductory note on differential privacy in my old
blog [1], but it was over 3 years ago. One could be forgiven in thinking that I should have become an expert
by now. Yeah, no. 

It was typical Apple not disclosing any information that would have justified the move as visionary, and that would
also have satisfied many security researchers. There are two ways to see it, and they are not mutually
exclusive. First, it's testament to how the technology has matured that it would soon become the norm. Second,
there are tremendous risks in adopting this technology without transparency, as it is teemed with
non-technological trade-offs. The most important consideration is how to trade privacy for utility. Of course
the fact that Apple even bring this consideration into a table is hugely encouraging, but this effort would be
but all for nothing if they did not make their decisions public. It is essential for the users to understand
the risks before sharing, and concealing them would completely offset the benefit of differential privacy. 

Though papers on differentially private algorithms and mechanisms still fill me with dreads, I have started to
comprehend and appreciate some of the very cool properties of differential privacy, thanks to two awesome
lecture notes I found online [2, 3]. The rest of this article I will try to articulate these properties as
much as I understand them. 

## Definition
Differential privacy is only applicable to *randomized algorithm* $$\cal{A}(\cal{D}^n) = \cal{Y}^k$$. It means
applying $$\cal{A}$$ on the same input will give different results, based on the whatever randomness embedded
in $$\cal{A}$$ (or its coin toss). It's easy to image that $$\cal{D}^n$$ is the database of $$n$$ rows, and
$$\cal{Y}^k$$ is the space of vectorized results $$\langle y_1, y_2, .., y_k \rangle$$.  

**Ideal privacy.** We want $$\cal{A}(.)$$ to not reveal information about a given individual $$x$$, given
arbitrary auxiliary information. This means _publishing_ the result of $$\cal{A}$$ does not violate any
privacy. This, however, is impossible to achieve. A counter example is that $$x$$ is an associate professor,
and $$\cal{A}(.)$$ on the database of university staff reveals that an average salary of an associate
professor is (approximately) $$100K$$. Thus, we know that $$x$$ salary is $$100K$$ in average --- new
information being revealed as the result is published.  

**Relaxed (differential) privacy.** The ideal privacy is impossible, meaning that performing an analysis task
on the data inevitably leads to leakage. Differential privacy attempts to relax the requirement in order to
beat the impossibility result. It requires analysis result to be almost the same whether $$x$$ is in
$$\cal{D}^n$$ or not. It means that $$x$$ can opt out of the analysis without affecting the result. It also
means that $$x$$'s privacy is not decreased if she is in $$\cal{D}^n$$. Hence the name _differential_. 

**Formal definition.** _Algorithm $$\cal{A}: \mathbb{D} \to \mathbb{Y}$$ is $$\epsilon$$-differentially
private if for all $$\cal{Y}^n \in \mathbb{Y}$$ and $$\cal{D}_1^n, \cal{D}_2^n \in \mathbb{D}$$ where
$$\cal{D}_1^n$$ and $$\cal{D}_2^n$$ differs by at most 1 entry, then:_

$$
\frac{Pr[\cal{A}(\cal{D}_1^n) = \cal{Y}^n]}{Pr[\cal{A}(\cal{D}_2^n) = \cal{Y}^n]} \leq e^\epsilon \qquad \quad (1)
$$

**Different notations.** Equation $$(1)$$ can be expanded or re-written as in the following ways:

+ When $$\epsilon$$ is small, $$e^\epsilon \approx 1+x$$. Thus:

$$
1 - \epsilon \leq \frac{Pr[\cal{A}(\cal{D}_1^n) = \cal{Y}^n]}{Pr[\cal{A}(\cal{D}_2^n) = \cal{Y}^n]} \leq 1 +
\epsilon \qquad \quad (2)
$$

+ Because $$Pr[f(X) = Y] = Pr[f(x) = Y \vert X=x]$$, we have:

$$
\frac{Pr[\cal{A}(\cal{D}^n = \cal{D}_1^n) = \cal{Y}^n]}{Pr[\cal{A}(\cal{D}_2^n) = \cal{Y}^n]} \leq e^\epsilon \qquad \quad (1)
$$

Goes here

## Security equivalence
Goes here

## Properties

### Sequential composition

### Partitioning

### Post processing

## Mechanisms

### No curator - randomized response

### Trusted curator 

+ Interactive vs. non-interactive
+ Laplace
+ Histogram, counts
+ Linear regression


[1] https://anhdinhdotme.wordpress.com/2013/03/26/on-differential-privacy/

[2] https://www.acsu.buffalo.edu/~gaboardi/teaching/cse711Spring2016/Lecture1.pdf

[3] http://people.eecs.berkeley.edu/~sltu/writeups/6885-lec20-b.pdf
