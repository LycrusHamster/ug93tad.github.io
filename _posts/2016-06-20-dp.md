---
layout: post
title: Another post about differential privacy 
---

In the wake of [Apple's
announcement](https://www.wired.com/2016/06/apples-differential-privacy-collecting-data/) that they
incorporating differential privacy in many of the Apple products, I think it's high time to continue my quest
to have a firm grasp on differential privacy. I wrote an introductory note on differential privacy in my old
blog [1], but it was over 3 years ago. One could be forgiven in thinking that I should have become an expert
by now. Yeah, no. 

It was typical Apple not disclosing any information that would have justified the move as visionary, and that would
also have satisfied many security researchers. There are two ways to see it, and they are not mutually
exclusive. First, it's testament to how the technology has matured that it would soon become the norm. Second,
there are tremendous risks in adopting this technology without transparency, as it is teemed with
non-technological trade-offs. The most important consideration is how to trade privacy for utility. Of course
the fact that Apple even bring this consideration into a table is hugely encouraging, but this effort would be
but all for nothing if they did not make their decisions public. It is essential for the users to understand
the risks before sharing, and concealing them would completely offset the benefit of differential privacy. 

Though papers on differentially private algorithms and mechanisms still fill me with dreads, I have started to
comprehend and appreciate some of the very cool properties of differential privacy, thanks to two awesome
lecture notes I found online [2, 3]. The rest of this article I will try to articulate these properties as
much as I understand them. 

## Definition
Differential privacy is only applicable to *randomized algorithm* $$\cal{A}(\cal{D}^n) = \cal{Y}^k$$ ($$\cal{Y}^k$$ can
be a probability space, hence the correct formula is $$\cal{A}(\cal{D}^n) \in \cal{Y}^k$$). It means applying $$\cal{A}$$
on the same input will give different results, based on the whatever randomness embedded in $$\cal{A}$$ (or its coin
toss).  It's easy to image that $$\cal{D}^n$$ is the database of $$n$$ rows, and $$\cal{Y}^k$$ is the space of
vectorized results $$\langle y_1, y_2, .., y_k \rangle$$.  

**Ideal privacy.** We want $$\cal{A}(.)$$ to not reveal information about a given individual $$x$$, given
arbitrary auxiliary information. This means _publishing_ the result of $$\cal{A}$$ does not violate any
privacy. This, however, is impossible to achieve. A counter example is that $$x$$ is an associate professor,
and $$\cal{A}(.)$$ on the database of university staff reveals that an average salary of an associate
professor is (approximately) $$100K$$. Thus, we know that $$x$$ salary is $$100K$$ in average --- new
information being revealed as the result is published.  

**Relaxed (differential) privacy.** The ideal privacy is impossible, meaning that performing an analysis task
on the data inevitably leads to leakage. Differential privacy attempts to relax the requirement in order to
beat the impossibility result. It requires analysis result to be almost the same whether $$x$$ is in
$$\cal{D}^n$$ or not. It means that $$x$$ can opt out of the analysis without affecting the result. It also
means that $$x$$'s privacy is not decreased if she is in $$\cal{D}^n$$. Hence the name _differential_. 

**Formal definition.** _Algorithm $$\cal{A}: \mathbb{D} \to \mathbb{Y}$$ is $$\epsilon$$-differentially
private if for all $$\cal{Y}^n \in \mathbb{Y}$$ and $$\cal{D}_1^n, \cal{D}_2^n \in \mathbb{D}$$ where
$$\cal{D}_1^n$$ and $$\cal{D}_2^n$$ differs by at most 1 entry, then:_

$$
\frac{Pr[\cal{A}(\cal{D}_1^n) = {\cal Y}^n]}{Pr[\cal{A}({\cal D}_2^n) = {\cal Y}^n]} \leq e^\epsilon \qquad \quad (1)
$$

**Different notations.** Equation $$(1)$$ can be expanded or re-written as in the following ways:

+ When $$\epsilon$$ is small, $$e^\epsilon \approx 1+x$$. Thus:

$$
1 - \epsilon \leq \frac{Pr[\cal{A}(\cal{D}_1^n) = \cal{Y}^n]}{Pr[\cal{A}(\cal{D}_2^n) = \cal{Y}^n]} \leq 1 +
\epsilon \qquad \quad (2)
$$

+ Because $$Pr[f(.) = y \;\vert\; X=x ] = Pr[f(x) = y]$$, we have:

$$
\frac{Pr[\cal{A}(.) = \cal{Y}^n \;\vert\; \cal{D}^n = \cal{D}_1^n]}{Pr[\cal{A}(.) = \cal{Y}^n \;\vert\; 
\cal{D}^n = \cal{D}_2^n]} \leq e^\epsilon \qquad \quad (3)
$$

Differential privacy written in Equation (3) be stated as: the probably of $$\cal{A}(.)$$ outputting the same value
_given_ the input $$\cal{D}_1^n$$ is _similar_ to that given the input $$\cal{D}_2^n$$.


## Security equivalence
The formal definition (Eq. 2) expresses the bound of leakage in information theoretic sense, which is useful in most
statistical analysis. Most mathematicians or statisticians can work comfortably with this definition. For people coming
from a security background like me, a definition based on a security game would be easier to grasp. And I was glad to
finally find such a definition in [2]. It was truly refreshing seeing the same problem in a new light. 

The game, as usual, involves a challenger (Alice) and an adversary Bob. It proceeds as follows:

1. Alice generates $${\cal D}^n = \langle x_1,x_2,..,x_n \rangle$$.  

2. Alice sends $$\cal{A}$$, $${\cal D}^{n-1} = \langle x_1,x_2,..,x_{n-1} \rangle$$ and $$\cal{Y}^k = {\cal A}({\cal D}^n)$$ to Bob.

3. Bob guesses $$x_n$$. 

4. Bob wins if it can guess $$x_n$$ correctly with a probability __greater__ than a random guess. 

The winning probability of Bob is the level of privacy decrease that $$x_n$$ suffers, i.e. the advantage the adversary
would have had in identifying $$x_n$$ in the dataset. We can prove that if $$\cal{A}(.)$$ is $$\epsilon$$-differentially
private, the probability of Bob winning the above game is bound by $$\epsilon$$. Let $$\cal{D}_j^n$$ denote the data
$$\cal{D}$$ where $$x_n=j$$. For simplicity, we make several assumptions:

1. (**A1**) $$x_i \in \{0,1\}$$ 

2. (**A2**) Alice chooses $$x_n$$ by flipping an unbiased coin, i.e. $$Pr[x_n=0] = Pr[x_n=1] = \frac{1}{2}$$. 

2. (**A3**) $$\cal{Y}^k \in \{0,1\}$$. 

3. (**A4**) $$Pr[{\cal A}({\cal D}_0^n) = 0] > Pr[{\cal A}({\cal D}_1^n) = 0]$$. 

We have:

$$
\begin{align*}
Pr[\text{Bob wins}] & =  Pr[\text{guess}\ 0, x_n=0] + Pr[\text{guess}\ 1, x_n=1] \\
& = \frac{1}{2} Pr[\text{guess}\ 0 \,\vert\, x_n=0] + \frac{1}{2} Pr[\text{guess}\ 1 \,\vert\, x_n=1] \qquad (4) \\
& = \frac{1}{2} Pr[{\cal A}({\cal D}_0^n) = 0] + \frac{1}{2} Pr [{\cal A}({\cal D}_1^n) = 1] \qquad (5) \\ 
& = \frac{1}{2} + \frac{1}{2} (Pr[{\cal A}({\cal D}_0^n) = 0] - Pr [{\cal A}({\cal D}_1^n) = 0]) \\ 
& \leq \frac{1}{2} + \frac{\epsilon}{2} \qquad (6)
\end{align*}
$$

The derivation from Eq.4 to Eq.5 is because the adversary's (best) guessing strategy is to compute $${\cal A}(.)$$ with
all values of $$x_n$$ and picks the one maximizing $$Pr[{\cal A}(.) = {\cal Y}^k]$$. Thus, the probably of
guessing $$x_n=0$$ is the same probability $$Pr[{\cal A}({\cal D}_0^n) = 0]$$ due to assumption A4. 

Eq.5 suggests that if $$\epsilon$$ is small, so is the leakage. In particular, if $$\epsilon = neg(.)$$ for a negligible
function $$neg(.)$$, the definition reduces to the _indistinguishibility_ notion found in traditional cryptography.
In practice, however, $$\epsilon$$ is quite substantial (in the range of $$(0.1, 10]$$) for the analysis to be useful.
Thus, there is a real risk of decrease in privacy when an individual is included in the data.  


## Properties

### Sequential composition

### Partitioning

### Post processing

## Mechanisms

### No curator - randomized response

### Trusted curator 

+ Interactive vs. non-interactive
+ Laplace
+ Histogram, counts
+ Linear regression


[1] https://anhdinhdotme.wordpress.com/2013/03/26/on-differential-privacy/

[2] https://www.acsu.buffalo.edu/~gaboardi/teaching/cse711Spring2016/Lecture1.pdf

[3] http://people.eecs.berkeley.edu/~sltu/writeups/6885-lec20-b.pdf
